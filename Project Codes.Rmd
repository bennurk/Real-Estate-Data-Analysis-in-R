---
title: "STAT467"
author: "Bennur Kaya"
date: "28 Aralık 2019"
output: word_document
---


satilik=read.csv("satilik.csv", header=TRUE, sep=",")
satilikbinary=read.csv("satilikbinary.csv", header=TRUE, sep=",")
attach(satilik)
summary(satilik[,c(1:5,8:10)])
attach(satilikbinary)

#correlation
library(corrplot)
library(car)
cor=cor(na.omit(satilik[,c(1:5,8:10)]))
corrplot(cor, method="number")
cor.test(Price,Sqm)

#standardize
st.data=as.data.frame(scale(na.omit(satilik[,c(1:5,8:10)]))) # standardize the variables
summary(st.data)

hist(st.data[,"Price"],xlab="price",main="Histogram of Price",col="lightblue3")
library(MASS)
#To make all values positive
min(st.data[,"Price"])
a=satilik$Price

library(MASS)
Box = boxcox(a ~ 1)
Cox = data.frame(Box$x, Box$y) # Create a data frame with the results
Cox2 = Cox[with(Cox, order(-Cox$Box.y)),] # Order the new data frame by decreasing y
Cox2[1,] # Display the lambda with the greatest
lambda = Cox2[1, "Box.x"]
T_box = ((a ^ lambda) - 1) /lambda # Transform the original data
T_box

hist(T_box,main="Histogram of Transformed
Price",
xlab="Transformed Price", col="salmon")
shapiro.test(T_box)

#PCA
data.pca=prcomp(st.data) # do a PCA
data.pca
summary(data.pca)
library(factoextra)

eig.val <- get_eigenvalue(data.pca)
eig.val

fviz_eig(data.pca, addlabels = TRUE, ylim = c(0,45))

#biplot
pcaCharts <- function(x) {
x.var <- x$sdev ^ 2
x.pvar <- x.var/sum(x.var)
print("proportions of variance:")
print(x.pvar)
par(mfrow=c(2,2))
plot(x.pvar,xlab="Principal component", ylab="Proportion of variance explained", ylim=c(0,1), type='b')
plot(cumsum(x.pvar),xlab="Principal component", ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b')
screeplot(x)
screeplot(x,type="l")
par(mfrow=c(1,1)) }
pcaCharts(data.pca)
biplot(data.pca,scale=0, cex=.7)

#Dividing data into train and test#


library(caTools)
set.seed(123)#Set seed to ensure you always have same random numbers generated.

sample=sample.split(st.data,SplitRatio = 0.80)
traindata=subset(st.data,sample ==TRUE)
testdata=subset(st.data, sample==FALSE)


#satilikbinary datası için train-test set

set.seed(123)#Set seed to ensure you always have same random numbers generated.

library(MASS)
smp_size<- floor(0.75 * nrow(satilikbinary))
train_ind <- sample(nrow(satilikbinary), size = smp_size)
newtrain <- as.data.frame(satilikbinary[train_ind, ])
newtest <- as.data.frame(satilikbinary[-train_ind, ])

#PCA
prin_comp=prcomp(traindata) 
prin_comp$rotation #The rotation measure provides the principal component loading.
summary(prin_comp) #Proportion of variance explains _% of the total variance. 
screeplot(prin_comp, type="lines")
biplot(prin_comp, scale = 0)
cor(prin_comp$x)#To check they are orthogonal.

library(rpart)
pc<-princomp(traindata[,1:4],cor=TRUE)
fviz_eig(prin_comp, addlabels = TRUE, ylim = c(0,45))

#PCA MODEL
na_traindata=na.omit(traindata)
model.pca<-lm(na_traindata$Price~pc$scores[,1]+pc$scores[,2]+pc$scores[,3]+pc$scores[,4],data=na_traindata)
plot(model.pca)
summary(model.pca) 

res.pca<-resid(model.pca)
qqnorm(res.pca)
qqline(res.pca)
shapiro.test(res.pca)
library(car)
ncvTest(model.pca) #H0:Constant Variance

#PCA Cluster
set.seed(123)
resvar <- get_pca_var(data.pca)
resvar$contrib # Contributions to the PCs
resvar$cos2 # Quality of representation

# Contributions of variables on PC1
fviz_contrib(data.pca, choice = "var", axes = 1)
# Contributions of variables on PC2
fviz_contrib(data.pca, choice = "var", axes = 2)
# Contributions of variables on PC1-2
fviz_contrib(data.pca, choice = "var", axes = 1:2)

# Color variables by groups
fviz_pca_var(data.pca, 
             palette = c("#0073C2FF", "#EFC000FF"),
             legend.title = "Cluster")

biplot(data.pca)

#Factor Analysis
library(devtools)
library(psych)
library(GPArotation)
twofactor <- factanal(st.data, 2,rotation = "varimax")
print(twofactor)
print(twofactor$loadings,cutoff = 0.33)

fit = factanal(st.data, 4, rotation = "varimax")
print(fit, digits=2, cutoff=.3, sort=TRUE)
# plot factor 1 by factor 2 
load <- fit$loadings[,1:3]

#Clustering
library("factoextra")
fviz_nbclust(st.data, kmeans,method = "silhouette")

satilik=na.omit(satilik)
km.res=kmeans(st.data, 2, nstart = 25)

aggregate(st.data, by=list(cluster=km.res$cluster), mean)
satilikd=cbind(st.data, cluster=km.res$cluster)
head(satilikd) #clustera göre ekstra column

#Cluster means
km.res$centers
km.res <- kmeans(x = st.data, centers = 2, nstart = 25)
fviz_cluster(object = km.res, data = st.data, geom = "point", stand = FALSE, 
             ellipse.type = "norm") + theme_bw()


#LDA
newtrain=na.omit(newtrain)
newtest=na.omit(newtest)
library(MASS)
f=paste(names(newtrain)[1],"~",paste(names(newtrain)[-1],collapse = "+"))
satiliklda=lda(as.formula(paste(f)),data = newtrain)
satiliklda=na.omit(satiliklda)
satiliklda.predict=predict(satiliklda, newdata=newtest)
table(predict(satiliklda, type="class")$class, newtrain$Price1)

#Logistic Regression
na.omit(satilikbinary)
fit<-glm(Price1~Room+Salon+Sqm+Age+Bathroom+Floor+FloorCount+as.factor(Heating)+as.factor(Type)+as.factor(Fuel)+as.factor(Build)+as.factor(Register)+as.factor(city)+as.factor(Furnished)+as.factor(BuildState)+as.factor(Usage), data =na.omit(satilikbinary), family = "binomial")

summary(fit)

#Multicollinearity
library(car)
vif(fit)

#Selection
stepAIC(fit, direction = c("both", "backward", "forward"))

new_fit= glm(formula = Price1 ~ Room + Sqm + Age + Bathroom + Floor + as.factor(Heating) + 
    as.factor(Register) + as.factor(city) + as.factor(BuildState) + 
    as.factor(Usage), family = "binomial", data = na.omit(satilikbinary))
summary(new_fit)

plot(new_fit)
shapiro.test(new_fit$residuals)

library(lmtest)
bptest(new_fit) #H0:Constant Variance

#model adequacy check
threshold=0.5
predicted_values=ifelse(predict(new_fit,type="response")>threshold,1,0)
actual_values=new_fit$y
conf_matrix=table(predicted_values,actual_values)
conf_matrix

library(caret)
sensitivity(conf_matrix)
specificity(conf_matrix)
#ikisi de yüksek

missclassificationrate=(166+210)/1938
missclassificationrate
#Decision Tree
library(rpart)
library(rpart.plot)

tree=rpart(Price1~Room + Sqm + Age + Bathroom + Floor + as.factor(Heating) + 
    as.factor(Register) + as.factor(city) + as.factor(BuildState) + 
    as.factor(Usage),data=newtrain,method="class")
tree
rpart.plot(tree,extra=106)

#Decision Tree Model
library(caret)
set.seed(3333)
# fitting decision tree classification model
newtrain$Price1 <- as.factor(newtrain$Price1)
set.seed(3333)
# fitting decision tree classification model
newtrain<-  na.omit(newtrain)
levels(newtrain$Price1) <- c("firstclass", "secondclass")

# control parameters
trctrl <- trainControl(method = "cv", n = 10, classProbs = TRUE)
DTModel <- train(Price1 ~ Room + Sqm + Age + Bathroom + Floor + as.factor(Heating) + 
                   as.factor(Register) + as.factor(city) + as.factor(BuildState) + 
                   as.factor(Usage),data=newtrain, method = "rpart", metric = "ROC",
                 parms  = list(split = "gini"), 
                 trControl = trctrl)
# model summary
DTModel

# viasulaziation
library(rpart.plot)
prp(DTModel$finalModel, box.palette = "Reds", tweak = 1.2, varlen = 20)

# plotting variable importance
plot(varImp(DTModel))
	

#Prediction with 0.5 cut-off
library(e1071)
# predicting the model on test data set
PredDTModel <- predict(DTModel, newdata = newtest,type = "prob")
summary(PredDTModel)

# taking the cut-off probability 50%
pred.DT <- ifelse(PredDTModel$firstclass > 0.5, "0", "1")
# saving predicted vector as factor 
Pred <- as.factor(pred.DT)
# ordering the vectors
Predicted <- ordered(Pred, levels = c("0", "1"))
Actual <- ordered(newtest$Price1,levels = c("0", "1"))
# making confusion matrix
cm <-confusionMatrix(table(Predicted,Actual))
cm

#Prediction with 0.4 cut-off
library(e1071)
# predicting the model on test data set
PredDTModel <- predict(DTModel, newdata = newtest,type = "prob")
summary(PredDTModel)
# taking the cut-off probability 40%
pred.DT <- ifelse(PredDTModel$firstclass > 0.4, "0", "1")
# saving predicted vector as factor 
Pred <- as.factor(pred.DT)
# ordering the vectors
Predicted <- ordered(Pred, levels = c("0", "1"))
Actual <- ordered(newtest$Price1,levels = c("0", "1"))
# making confusion matrix
Cm2 <-confusionMatrix(table(Predicted,Actual))
Cm2

#Information Gain
library(rJava)
library(FSelector)
infogain=information.gain(Price1 ~ Room + Sqm + Age + Bathroom + Floor + as.factor(Heating) + 
    as.factor(Register) + as.factor(city) + as.factor(BuildState) + 
    as.factor(Usage), data=satilikbinary, unit="log")
print(infogain)
gainrat=gain.ratio(Price1 ~ Room + Sqm + Age + Bathroom + Floor + as.factor(Heating) + 
    as.factor(Register) + as.factor(city) + as.factor(BuildState) + 
    as.factor(Usage), data=satilikbinary)
print(gainrat)

#RANDOM FOREST
rf <- randomForest(
num ~ .,
data=train
)
rf
plot(rf)

# Create features and target
nasatilik=na.omit(satilik)
X <- satilik[,-1]
y <- satilik$Price

#splitting data
index <- createDataPartition(y, p=0.8, list=FALSE)
X_train <- X[ index, ]
X_test <- X[-index, ]
y_train <- y[index]
y_test<-y[-index]

rf_fit35=randomForest(X_train,y_train,importance=TRUE)

importance(rf_fit35)


# Train the model 
regr <- randomForest(x = X_train, y = y_train , maxnodes = 10, ntree = 10)

# Make prediction
predictions <- predict(regr, X_test)

result <- X_test
result['Price'] <- y_test
result['prediction']<-  predictions

head(result)

# Import library for visualization
library(ggplot2)

# Build scatterplot
ggplot(  ) + 
  geom_point( aes(x = X_test$Age, y = y_test, color = 'red', alpha = 0.5) ) + 
  geom_point( aes(x = X_test$Age , y = predictions, color = 'blue',  alpha = 0.5)) + 
  labs(x = "Age", y = "Price", color = "", alpha = 'Transperency') +
  scale_color_manual(labels = c( "Predicted", "Real"), values = c("blue", "red")) 

library(Metrics)

print(paste0('MAE: ' , mae(y_test,predictions) ))

print(paste0('MSE: ' ,caret::postResample(predictions , y_test)['RMSE']^2 ))

print(paste0('R2: ' ,caret::postResample(predictions , y_test)['Rsquared'] ))

#Tuning parameters

# If training the model takes too long try setting up lower value of N
N=length(X_train)
X_train_ = X_train[1:N,]
y_train_ = y_train[1:N]

seed <-7
metric<-'RMSE'

customRF <- list(type = "Regression", library = "randomForest", loop = NULL)

customRF$parameters <- data.frame(parameter = c("maxnodes", "ntree"), class = rep("numeric", 2), label = c("maxnodes", "ntree"))

customRF$grid <- function(x, y, len = NULL, search = "grid") {}

customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, maxnodes = param$maxnodes, ntree=param$ntree, ...)
}

customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes

# Set grid search parameters
control <- trainControl(method="repeatedcv", number=10, repeats=3, search='grid')

# Outline the grid of parameters
tunegrid <- expand.grid(.maxnodes=c(70,80,90,100), .ntree=c(900, 1000, 1100))
set.seed(seed)

# Train the model
rf_gridsearch <- train(x=X_train_, y=y_train_, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)

plot(rf_gridsearch)


#Best Parameters
rf_gridsearch$bestTune

#Defining and visualizing variables importance
varImpPlot(rf_gridsearch$finalModel, main ='Feature importance', col="salmon")

#Canonical Analysis
library(ggplot2)
library(GGally)
library(CCA)
library(CCP)
group1 = satilik[,c(1,5,7,9,10)] #price,floor,floorcount,age,type
group2 = satilik[,c(2,3,4,8,14)]  

ggpairs(na.omit(group1))
ggpairs(na.omit(group2# correlations
matcor(group1, group2)

# R Canonical Correlation Analysis
cc1 <- cc(group1,group2)

#display the canonical correlations
cc1$cor

#raw canonical coefficients
cc1[3:4] 
# compute canonical loadings
cc2 <- comput(group1, group2, cc1)
# display canonical loadings
cc2[3:6] 
# tests of canonical dimensions
rho <- cc1$cor
## Define number of observations, number of variables in first set, and number of variables in the second set.
n <- dim(group1)[1] 
p <- length(group1)
q <- length(group2)
## Calculate p-values using the F-approximations of different test statistics:
library(CCP)
p.asym(rho, n, p, q, tstat = "Wilks")
p.asym(rho, n, p, q, tstat = "Hotelling")
p.asym(rho, n, p, q, tstat = "Pillai")
p.asym(rho, n, p, q, tstat = "Roy")

# standardized psych canonical coefficients diagonal matrix of psych sd's
s1 <- diag(sqrt(diag(cov(na.omit(group1)))))
s1 %*% cc1$xcoef

# standardized acad canonical coefficients diagonal matrix of acad sd's
s2 <- diag(sqrt(diag(cov(na.omit(group2)))))
s2 %*% cc1$ycoef


